{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "In this exercise, we'll implement a 'vanilla' recurrent neural network and apply it to a character-level language modeling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup and load data\n",
    "import numpy as np\n",
    "\n",
    "data = file('data/input.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size = 100, hidden_size = 100):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_xh = np.random.randn(self.hidden_size, self.input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(self.hidden_size, self.hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(self.input_size, self.hidden_size) * 0.01\n",
    "        self.b_hh = np.zeros((self.hidden_size, 1))\n",
    "        self.b_hy = np.zeros((self.input_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "Let's implement the forward pass and cross-entropy loss calculation given by the following equations.\n",
    "\n",
    "$$ \\mathbf{h_t} = \\tanh({W_{xh}\\mathbf{x_t} + W_{hh}\\mathbf{h_{t-1}}}) $$\n",
    "\n",
    "$$ \\mathbf{p_t} = softmax(W_{hy}\\mathbf{h_t}) $$\n",
    "\n",
    "$$ L = - \\sum y_n\\log(p_n) $$\n",
    "\n",
    "Here, $ \\mathbf{x_t} $ is the 1-of-k encoded input at time $ t $, $ \\mathbf{h_t} $ is the hidden state at time $ t $ and $ \\mathbf{p_t} $ is the output character probabilities at time $ t $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(self, inputs, targets, hprev):\n",
    "    xs = {} # 1-of-k encoded input\n",
    "    hs = {} # Hidden state\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ys = {} # Unnormalized log probabilities for next characters\n",
    "    ps = {} # Probabilities of next characters\n",
    "    loss = 0\n",
    "    \n",
    "    # Iterate through every character in input, and\n",
    "    # calculate xs, hs, ys, ps and cross-entropy loss\n",
    "    # TODO\n",
    "    #\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    return xs, hs, ps, loss\n",
    "\n",
    "RNN.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if our implementation is correct, we can compare the actual loss with expected loss for random predictions. If we initialize with equal weights, actual and expected loss would be exactly equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss for random predictions: 115.378013\n",
      "Actual loss: 115.379292\n"
     ]
    }
   ],
   "source": [
    "seq_length = 25\n",
    "\n",
    "inputs = [char_to_ix[ch] for ch in data[0:seq_length]]\n",
    "targets = [char_to_ix[ch] for ch in data[1:seq_length+1]]\n",
    "\n",
    "r = RNN(len(chars))\n",
    "\n",
    "loss_expected = -np.log(1.0/r.input_size) * seq_length\n",
    "_, _, _, loss_actual = r.forward(inputs, targets, np.zeros((r.hidden_size, 1)))\n",
    "\n",
    "print \"Expected loss for random predictions: %f\" % loss_expected\n",
    "print \"Actual loss: %f\" % loss_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "Next, we implement the backward pass. This is just application of the chain rule. One thing to keep in mind is that since RNN parameters are shared across time, gradient at each output depends not only on the current time step, but also the previous time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(self, input, targets, xs, hs, ps):\n",
    "    gW_xh, gW_hh, gW_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hh), np.zeros_like(self.W_hy)\n",
    "    gb_hh, gb_hy = np.zeros_like(self.b_hh), np.zeros_like(self.b_hy)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "    # Iterate through every character in reversed input, and\n",
    "    # write backpropagation step\n",
    "    # TODO\n",
    "    #\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    # Clipping gradients\n",
    "    for gparam in [gW_xh, gW_hh, gW_hy, gb_hh, gb_hy]:\n",
    "        np.clip(gparam, -5, 5, out = gparam)\n",
    "\n",
    "    return gW_xh, gW_hh, gW_hy, gb_hh, gb_hy\n",
    "\n",
    "RNN.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to compare the analytic and numerical gradients to check if our backpropagation is implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.006931, -0.006931 => 2.454077e-09 \n",
      "-0.000545, -0.000309 => 2.763630e-01 \n",
      "0.000704, 0.000710 => 3.867996e-03 \n",
      "0.005532, 0.005532 => 1.387140e-05 \n",
      "0.247394, 0.247392 => 3.887931e-06 \n"
     ]
    }
   ],
   "source": [
    "seq_length = 25\n",
    "\n",
    "inputs = [char_to_ix[ch] for ch in data[0:seq_length]]\n",
    "targets = [char_to_ix[ch] for ch in data[1:seq_length+1]]\n",
    "\n",
    "r = RNN(len(chars))\n",
    "\n",
    "delta = 1e-5\n",
    "\n",
    "xs, hs, ps, loss_1 = r.forward(inputs, targets, np.zeros((r.hidden_size, 1)))\n",
    "gW_xh, gW_hh, gW_hy, gb_hh, gb_hy = r.backward(inputs, targets, xs, hs, ps)\n",
    "hprev = hs[len(inputs)-1]\n",
    "\n",
    "for param, gparam in zip([r.W_xh, r.W_hh, r.W_hy, r.b_hh, r.b_hy],\n",
    "                         [gW_xh, gW_hh, gW_hy, gb_hh, gb_hy]):\n",
    "    ix = np.random.randint(0, param.size)\n",
    "    old_val = param.flat[ix]\n",
    "    \n",
    "    param.flat[ix] = old_val + delta\n",
    "    xs, hs, ps, loss_1 = r.forward(inputs, targets, hprev)\n",
    "    _, _, _, _, _ = r.backward(inputs, targets, xs, hs, ps)\n",
    "    \n",
    "    param.flat[ix] = old_val - delta\n",
    "    xs, hs, ps, loss_2 = r.forward(inputs, targets, hprev)\n",
    "    _, _, _, _, _ = r.backward(inputs, targets, xs, hs, ps)\n",
    "    \n",
    "    param.flat[ix] = old_val\n",
    "    \n",
    "    grad_analytic = gparam.flat[ix]\n",
    "    grad_numerical = (loss_1 - loss_2) / (2 * delta)\n",
    "    rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "    print '%f, %f => %e ' % (grad_numerical, grad_analytic, rel_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and Beam search\n",
    "\n",
    "Given the distribution of character probabilities at each time step, there are several methods to obtain a single character that gets fed back into the RNN as input at the next time step.\n",
    "\n",
    "1. Taking the maximum at the current time step.\n",
    "2. Sampling from the distribution given by the softmax.\n",
    "3. Taking the top k and implementing beam search.\n",
    "\n",
    "Here, we provide a completed implementation of (2) to sample from the distribution. Fill in the missing code to implement beam search (3).\n",
    "\n",
    "Beam Search is a search algorithm similar to Breadth First Search (BFS). More details can be found [here](http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/46927-f97/slides/Lec3/sld023.htm) and [here](https://www.youtube.com/watch?v=G_teUutyC3Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(self, seed_ix, n, hprev):\n",
    "    h = np.copy(hprev)\n",
    "    x = np.zeros((self.input_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for i in xrange(n):\n",
    "        h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_hh)\n",
    "        y = np.dot(self.W_hy, h) + self.b_hy\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(self.input_size), p=p.ravel())\n",
    "        x = np.zeros((self.input_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "RNN.sample = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_search(self, seed_ix, n, hprev, beam_size = 5):\n",
    "    h = np.copy(hprev)\n",
    "    beams = [(0.0, [], h)] # log probabilities, indices, hidden state\n",
    "    while n:\n",
    "        # Select the top k characters from the distribution,\n",
    "        # and maintain a buffer of the respective hidden states\n",
    "        # TODO\n",
    "        # beam_candidates = []\n",
    "        # for b in beams:\n",
    "        #\n",
    "        # beams = beam_candidates[:beam_size]\n",
    "        # END OF YOUR CODE\n",
    "        n -= 1\n",
    "    samples = [b[1] for b in beams]\n",
    "    return samples\n",
    "\n",
    "RNN.beam_search = beam_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we put them together!\n",
    "\n",
    "### Data\n",
    "\n",
    "Input: A sequence of characters (indices correspoding to the characters in the vocabulary) sampled from the input text file of length `seq_length`\n",
    "\n",
    "Target: Input sequence shifted by 1 and sampled from input text ie. if input is `x[t]`, then target is `x[t+1]` where `t` is iterating over the characters in the input text file. \n",
    "\n",
    "The task is set as a classification task where given the index of the current character, the model learns to predict the index of the next character from the vocabulary. \n",
    "\n",
    "### Model \n",
    "\n",
    "As we are using a 1-hot representation of the input, the input size of the RNN corresponds to the size of the vocabulary. Increasing the hidden size increases the number of parameters to learn. \n",
    "\n",
    "### Training\n",
    "\n",
    "In the *train* method. We use the [**AdaGrad**](http://cs231n.github.io/neural-networks-3/#ada) update to tweak our parameters. Typically, [perplexity](https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf) is used as a measure the capability of the model to correctly predict the next sample. In this version only the softmax loss is calculated and used to decide the 'better' model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(self, data, learning_rate = 1e-1, seq_length = 25):\n",
    "    n = 0\n",
    "    p = 0\n",
    "    mW_xh, mW_hh, mW_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hh), np.zeros_like(self.W_hy)\n",
    "    mb_hh, mb_hy = np.zeros_like(self.b_hh), np.zeros_like(self.b_hy)\n",
    "    smooth_loss = -np.log(1.0/self.input_size)*seq_length\n",
    "    while True:\n",
    "        if p + seq_length >= len(data) or n == 0:\n",
    "            hprev = np.zeros((self.hidden_size, 1))\n",
    "            p = 0\n",
    "\n",
    "        inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "        targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "        if n % 100 == 0:\n",
    "            samples = self.beam_search(inputs[0], seq_length, hprev, 5)\n",
    "            for sample_ix in samples:\n",
    "                txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "                print '----\\n %s \\n----' % (txt)\n",
    "\n",
    "        xs, hs, ps, loss = self.forward(inputs, targets, hprev)\n",
    "        gW_xh, gW_hh, gW_hy, gb_hh, gb_hy = self.backward(inputs, targets, xs, hs, ps)\n",
    "\n",
    "        hprev = hs[len(inputs)-1]\n",
    "\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "        if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss)\n",
    "\n",
    "        for param, gparam, mem in zip([self.W_xh, self.W_hh, self.W_hy, self.b_hh, self.b_hy],\n",
    "                            [gW_xh, gW_hh, gW_hy, gb_hh, gb_hy],\n",
    "                            [mW_xh, mW_hh, mW_hy, mb_hh, mb_hy]):\n",
    "            mem += gparam * gparam\n",
    "            param += -learning_rate * gparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "        p += seq_length\n",
    "        n += 1\n",
    "\n",
    "RNN.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = RNN(len(chars), 200)\n",
    "r.train(data, learning_rate = 1e-2, seq_length = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
